{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Tibame_NaiveBayes.ipynb","provenance":[],"authorship_tag":"ABX9TyMh70ZYy5JMg/tqw3mkIcDP"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":288},"id":"T0zY5tk-ssjK","executionInfo":{"status":"ok","timestamp":1609936635633,"user_tz":-480,"elapsed":1001,"user":{"displayName":"Paul Chen","photoUrl":"","userId":"15113937160468943492"}},"outputId":"95febd9e-1945-4263-d45f-37c9746abc69"},"source":["import pandas as pd\r\n","\r\n","columns = ['sent', 'class']\r\n","rows = []\r\n","\r\n","rows = [['This is my book', 'stmt'], \r\n","        ['They are novels', 'stmt'],\r\n","        ['have you read this book', 'question'],\r\n","        ['who is the author', 'question'],\r\n","        ['what are the characters', 'question'],\r\n","        ['This is how I bought the book', 'stmt'],\r\n","        ['I like fictions', 'stmt'],\r\n","        ['what is your favorite book', 'question']]\r\n","\r\n","training_data = pd.DataFrame(rows, columns=columns)\r\n","training_data"],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sent</th>\n","      <th>class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>This is my book</td>\n","      <td>stmt</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>They are novels</td>\n","      <td>stmt</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>have you read this book</td>\n","      <td>question</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>who is the author</td>\n","      <td>question</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>what are the characters</td>\n","      <td>question</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>This is how I bought the book</td>\n","      <td>stmt</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>I like fictions</td>\n","      <td>stmt</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>what is your favorite book</td>\n","      <td>question</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                            sent     class\n","0                This is my book      stmt\n","1                They are novels      stmt\n","2        have you read this book  question\n","3              who is the author  question\n","4        what are the characters  question\n","5  This is how I bought the book      stmt\n","6                I like fictions      stmt\n","7     what is your favorite book  question"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"Ri_RfslquEvi"},"source":["Count vector for statement class"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":168},"id":"ox3wbm6Rs6wA","executionInfo":{"status":"ok","timestamp":1609936702210,"user_tz":-480,"elapsed":1793,"user":{"displayName":"Paul Chen","photoUrl":"","userId":"15113937160468943492"}},"outputId":"cb0b3845-fbb3-48c9-929d-e22e8b605e1f"},"source":["from sklearn.feature_extraction.text import CountVectorizer\r\n","\r\n","stmt_docs = [row['sent'] for index,row in training_data.iterrows() if row['class'] == 'stmt']\r\n","\r\n","vec_s = CountVectorizer()\r\n","X_s = vec_s.fit_transform(stmt_docs)\r\n","tdm_s = pd.DataFrame(X_s.toarray(), columns=vec_s.get_feature_names())\r\n","\r\n","tdm_s"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>are</th>\n","      <th>book</th>\n","      <th>bought</th>\n","      <th>fictions</th>\n","      <th>how</th>\n","      <th>is</th>\n","      <th>like</th>\n","      <th>my</th>\n","      <th>novels</th>\n","      <th>the</th>\n","      <th>they</th>\n","      <th>this</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   are  book  bought  fictions  how  is  like  my  novels  the  they  this\n","0    0     1       0         0    0   1     0   1       0    0     0     1\n","1    1     0       0         0    0   0     0   0       1    0     1     0\n","2    0     1       1         0    1   1     0   0       0    1     0     1\n","3    0     0       0         1    0   0     1   0       0    0     0     0"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"LUQvYtZmuLpk"},"source":["Count vector for Question Class"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":168},"id":"UP9QdWVGtt22","executionInfo":{"status":"ok","timestamp":1609936859954,"user_tz":-480,"elapsed":1001,"user":{"displayName":"Paul Chen","photoUrl":"","userId":"15113937160468943492"}},"outputId":"be4cec42-f292-4834-94ac-3fa864379c9e"},"source":["q_docs = [row['sent'] for index,row in training_data.iterrows() if row['class'] == 'question']\r\n","\r\n","vec_q = CountVectorizer()\r\n","X_q = vec_q.fit_transform(q_docs)\r\n","tdm_q = pd.DataFrame(X_q.toarray(), columns=vec_q.get_feature_names())\r\n","\r\n","tdm_q"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>are</th>\n","      <th>author</th>\n","      <th>book</th>\n","      <th>characters</th>\n","      <th>favorite</th>\n","      <th>have</th>\n","      <th>is</th>\n","      <th>read</th>\n","      <th>the</th>\n","      <th>this</th>\n","      <th>what</th>\n","      <th>who</th>\n","      <th>you</th>\n","      <th>your</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   are  author  book  characters  favorite  ...  this  what  who  you  your\n","0    0       0     1           0         0  ...     1     0    0    1     0\n","1    0       1     0           0         0  ...     0     0    1    0     0\n","2    1       0     0           1         0  ...     0     1    0    0     0\n","3    0       0     1           0         1  ...     0     1    0    0     1\n","\n","[4 rows x 14 columns]"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"pJULJGWZuQX4"},"source":["Feature Counts for stmt class (12)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nJgePjkEt2Mr","executionInfo":{"status":"ok","timestamp":1609936918877,"user_tz":-480,"elapsed":927,"user":{"displayName":"Paul Chen","photoUrl":"","userId":"15113937160468943492"}},"outputId":"6942a4c9-fd01-44dd-fc03-92520465e8fd"},"source":["word_list_s = vec_s.get_feature_names();    \r\n","count_list_s = X_s.toarray().sum(axis=0) \r\n","freq_s = dict(zip(word_list_s,count_list_s))\r\n","freq_s"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'are': 1,\n"," 'book': 2,\n"," 'bought': 1,\n"," 'fictions': 1,\n"," 'how': 1,\n"," 'is': 2,\n"," 'like': 1,\n"," 'my': 1,\n"," 'novels': 1,\n"," 'the': 1,\n"," 'they': 1,\n"," 'this': 2}"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"toiDjetTuh8g"},"source":["Feature Counts for Question Class(14)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6ONWfjRZudH2","executionInfo":{"status":"ok","timestamp":1609937065128,"user_tz":-480,"elapsed":847,"user":{"displayName":"Paul Chen","photoUrl":"","userId":"15113937160468943492"}},"outputId":"6800842b-fc20-4748-a472-815dce55195b"},"source":["\r\n","word_list_q = vec_q.get_feature_names();    \r\n","count_list_q = X_q.toarray().sum(axis=0) \r\n","freq_q = dict(zip(word_list_q,count_list_q))\r\n","freq_q"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'are': 1,\n"," 'author': 1,\n"," 'book': 2,\n"," 'characters': 1,\n"," 'favorite': 1,\n"," 'have': 1,\n"," 'is': 2,\n"," 'read': 1,\n"," 'the': 2,\n"," 'this': 1,\n"," 'what': 2,\n"," 'who': 1,\n"," 'you': 1,\n"," 'your': 1}"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"4Vh4ikARu8WN"},"source":["Total Count of Features in the Training Set"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y5_oTgCwu7gu","executionInfo":{"status":"ok","timestamp":1609938530595,"user_tz":-480,"elapsed":807,"user":{"displayName":"Paul Chen","photoUrl":"","userId":"15113937160468943492"}},"outputId":"7a84cb23-6636-4de8-a2f7-5e4aa72997fd"},"source":["from sklearn.feature_extraction.text import CountVectorizer\r\n","\r\n","docs = [row['sent'] for index,row in training_data.iterrows()]\r\n","\r\n","vec = CountVectorizer()\r\n","X = vec.fit_transform(docs)\r\n","\r\n","total_features = len(vec.get_feature_names())\r\n","print ('total_features:', total_features)\r\n","vec.get_feature_names()"],"execution_count":16,"outputs":[{"output_type":"stream","text":["total_features: 21\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["['are',\n"," 'author',\n"," 'book',\n"," 'bought',\n"," 'characters',\n"," 'favorite',\n"," 'fictions',\n"," 'have',\n"," 'how',\n"," 'is',\n"," 'like',\n"," 'my',\n"," 'novels',\n"," 'read',\n"," 'the',\n"," 'they',\n"," 'this',\n"," 'what',\n"," 'who',\n"," 'you',\n"," 'your']"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"cJnWbYwfvTHc"},"source":["Total Count of Class"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PtIYJlhIvQmP","executionInfo":{"status":"ok","timestamp":1609937567655,"user_tz":-480,"elapsed":857,"user":{"displayName":"Paul Chen","photoUrl":"","userId":"15113937160468943492"}},"outputId":"f71280fd-5d20-43b2-a8c7-b760f1ddda45"},"source":["\r\n","total_cnts_features_s = count_list_s.sum(axis=0)\r\n","total_cnts_features_q = count_list_q.sum(axis=0)\r\n","print ('total_cnts_features_s：',total_cnts_features_s)\r\n","print ('total_cnts_features_q：：',total_cnts_features_q)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["total_cnts_features_s： 15\n","total_cnts_features_q：： 18\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qeymhQMbvfb1","executionInfo":{"status":"ok","timestamp":1609938612744,"user_tz":-480,"elapsed":992,"user":{"displayName":"Paul Chen","photoUrl":"","userId":"15113937160468943492"}},"outputId":"a27cd4a7-a1d6-4197-b530-2e0eb10d9e0d"},"source":["import nltk\r\n","nltk.download('punkt')\r\n","from nltk.tokenize import word_tokenize\r\n","new_sentence = 'what is the price of the book'\r\n","new_word_list = word_tokenize(new_sentence)\r\n","new_word_list"],"execution_count":17,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["['what', 'is', 'the', 'price', 'of', 'the', 'book']"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"c-QXQhFhxM60"},"source":["Laplace Smoothing Stmt Class"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2pZ-zSuav2Lx","executionInfo":{"status":"ok","timestamp":1609938795211,"user_tz":-480,"elapsed":829,"user":{"displayName":"Paul Chen","photoUrl":"","userId":"15113937160468943492"}},"outputId":"06f4fe8d-f45a-4689-c794-aa19556da036"},"source":["\r\n","prob_s_with_ls = []\r\n","for word in new_word_list:    \r\n","    if word in freq_s.keys():\r\n","        count = freq_s[word]\r\n","    else:\r\n","        count = 0\r\n","    print ('word:', word, \"count:\", count)\r\n","    prob_s_with_ls.append((count + 1)/(total_cnts_features_s + total_features))\r\n","dict(zip(new_word_list,prob_s_with_ls))"],"execution_count":18,"outputs":[{"output_type":"stream","text":["word: what count: 0\n","word: is count: 2\n","word: the count: 1\n","word: price count: 0\n","word: of count: 0\n","word: the count: 1\n","word: book count: 2\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["{'book': 0.08333333333333333,\n"," 'is': 0.08333333333333333,\n"," 'of': 0.027777777777777776,\n"," 'price': 0.027777777777777776,\n"," 'the': 0.05555555555555555,\n"," 'what': 0.027777777777777776}"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"aRkPMPK8xHQe"},"source":["Laplace Smoothing Question Class"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_QQJWcwYxDvd","executionInfo":{"status":"ok","timestamp":1609937734648,"user_tz":-480,"elapsed":912,"user":{"displayName":"Paul Chen","photoUrl":"","userId":"15113937160468943492"}},"outputId":"48a70daf-14de-4572-cf38-a68a6cef7139"},"source":["prob_q_with_ls = []\r\n","for word in new_word_list:\r\n","    if word in freq_q.keys():\r\n","        count = freq_q[word]\r\n","    else:\r\n","        count = 0\r\n","    prob_q_with_ls.append((count + 1)/(total_cnts_features_q + total_features))\r\n","dict(zip(new_word_list,prob_q_with_ls))"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'book': 0.07692307692307693,\n"," 'is': 0.07692307692307693,\n"," 'of': 0.02564102564102564,\n"," 'price': 0.02564102564102564,\n"," 'the': 0.07692307692307693,\n"," 'what': 0.07692307692307693}"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"ZcyKw7g0AcoQ"},"source":["Conditional Probability Given stmt with no Smoothing"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DJVUQ3KR_mxp","executionInfo":{"status":"ok","timestamp":1609941586288,"user_tz":-480,"elapsed":1515,"user":{"displayName":"Paul Chen","photoUrl":"","userId":"15113937160468943492"}},"outputId":"3afaf1ce-3799-4551-aa09-30c7b2909833"},"source":["prob_s = []\r\n","t_count = 0\r\n","for count in count_list_s:\r\n","  t_count += count\r\n","print ('total count:', t_count)\r\n","for count in count_list_s:\r\n","    prob_s.append (count/t_count)\r\n","dict(zip(word_list_s,prob_s))"],"execution_count":28,"outputs":[{"output_type":"stream","text":["total count: 15\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["{'are': 0.06666666666666667,\n"," 'book': 0.13333333333333333,\n"," 'bought': 0.06666666666666667,\n"," 'fictions': 0.06666666666666667,\n"," 'how': 0.06666666666666667,\n"," 'is': 0.13333333333333333,\n"," 'like': 0.06666666666666667,\n"," 'my': 0.06666666666666667,\n"," 'novels': 0.06666666666666667,\n"," 'the': 0.06666666666666667,\n"," 'they': 0.06666666666666667,\n"," 'this': 0.13333333333333333}"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"markdown","metadata":{"id":"OhSEjrrFAO4p"},"source":["Conditional Probability Given question without Smoothing"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bu_DZuaI8T_k","executionInfo":{"status":"ok","timestamp":1609941527648,"user_tz":-480,"elapsed":1304,"user":{"displayName":"Paul Chen","photoUrl":"","userId":"15113937160468943492"}},"outputId":"e685eea9-ebb6-4149-d81f-7a6cd8f30991"},"source":["prob_q = []\r\n","t_count = 0\r\n","for count in count_list_q:\r\n","  t_count += count\r\n","print ('total count:', t_count)\r\n","for count in count_list_q:\r\n","    prob_q.append (count/t_count)\r\n","dict(zip(word_list_q,prob_q))\r\n"],"execution_count":27,"outputs":[{"output_type":"stream","text":["total count: 18\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["{'are': 0.05555555555555555,\n"," 'author': 0.05555555555555555,\n"," 'book': 0.1111111111111111,\n"," 'characters': 0.05555555555555555,\n"," 'favorite': 0.05555555555555555,\n"," 'have': 0.05555555555555555,\n"," 'is': 0.1111111111111111,\n"," 'read': 0.05555555555555555,\n"," 'the': 0.1111111111111111,\n"," 'this': 0.05555555555555555,\n"," 'what': 0.1111111111111111,\n"," 'who': 0.05555555555555555,\n"," 'you': 0.05555555555555555,\n"," 'your': 0.05555555555555555}"]},"metadata":{"tags":[]},"execution_count":27}]}]}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 語音轉成翻譯文字服務操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Say something...\n",
      "RECOGNIZED 'en-US': Good morning.\n",
      "TRANSLATED into ja-JP: おはようございます。\n"
     ]
    }
   ],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "speech_key, service_region = \"7d1206ee3a30479db6fa9fdf0154a209\", \"southcentralus\"\n",
    "\n",
    "def translate_speech_to_text():\n",
    "\n",
    "    # Creates an instance of a speech translation config with specified subscription key and service region.\n",
    "    # Replace with your own subscription key and region identifier from here: https://aka.ms/speech/sdkregion\n",
    "    translation_config = speechsdk.translation.SpeechTranslationConfig(subscription=speech_key, region=service_region)\n",
    "\n",
    "    # Sets source and target languages.\n",
    "    # Replace with the languages of your choice, from list found here: https://aka.ms/speech/sttt-languages\n",
    "    fromLanguage = 'en-US'\n",
    "    toLanguage = 'ja-JP'\n",
    "    translation_config.speech_recognition_language = fromLanguage\n",
    "    translation_config.add_target_language(toLanguage)\n",
    "\n",
    "    # Creates a translation recognizer using and audio file as input.\n",
    "    recognizer = speechsdk.translation.TranslationRecognizer(translation_config=translation_config)\n",
    "\n",
    "    # Starts translation, and returns after a single utterance is recognized. The end of a\n",
    "    # single utterance is determined by listening for silence at the end or until a maximum of 15\n",
    "    # seconds of audio is processed. It returns the recognized text as well as the translation.\n",
    "    # Note: Since recognize_once() returns only a single utterance, it is suitable only for single\n",
    "    # shot recognition like command or query.\n",
    "    # For long-running multi-utterance recognition, use start_continuous_recognition() instead.\n",
    "    print(\"Say something...\")\n",
    "    result = recognizer.recognize_once()\n",
    "\n",
    "    # Check the result\n",
    "    if result.reason == speechsdk.ResultReason.TranslatedSpeech:\n",
    "        print(\"RECOGNIZED '{}': {}\".format(fromLanguage, result.text))\n",
    "        print(\"TRANSLATED into {}: {}\".format(toLanguage, result.translations['ja']))\n",
    "    elif result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "        print(\"RECOGNIZED: {} (text could not be translated)\".format(result.text))\n",
    "    elif result.reason == speechsdk.ResultReason.NoMatch:\n",
    "        print(\"NOMATCH: Speech could not be recognized: {}\".format(result.no_match_details))\n",
    "    elif result.reason == speechsdk.ResultReason.Canceled:\n",
    "        print(\"CANCELED: Reason={}\".format(result.cancellation_details.reason))\n",
    "        if result.cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "            print(\"CANCELED: ErrorDetails={}\".format(result.cancellation_details.error_details))\n",
    "\n",
    "translate_speech_to_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 語音轉成多國翻譯文字服務操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Say something...\n"
     ]
    },
    {
     "ename": "SystemError",
     "evalue": "<built-in function TranslationRecognizer_recognize_once> returned a result with an error set",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\envs\\python_azure\\lib\\site-packages\\azure\\cognitiveservices\\speech\\speech_py_impl.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m   9356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 9357\u001b[1;33m     \u001b[0m__setattr__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_swig_setattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTranslationRecognitionResult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   9358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-3ef2d8f5dc97>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"CANCELED: ErrorDetails={}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcancellation_details\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_details\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m \u001b[0mtranslate_speech_to_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-22-3ef2d8f5dc97>\u001b[0m in \u001b[0;36mtranslate_speech_to_text\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;31m# For long-running multi-utterance recognition, use start_continuous_recognition() instead.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Say something...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecognizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecognize_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m# Check the result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python_azure\\lib\\site-packages\\azure\\cognitiveservices\\speech\\translation.py\u001b[0m in \u001b[0;36mrecognize_once\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mresult\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msynchronous\u001b[0m \u001b[0mrecognition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m         \"\"\"\n\u001b[1;32m--> 246\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mTranslationRecognitionResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecognize_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrecognize_once_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mResultFuture\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python_azure\\lib\\site-packages\\azure\\cognitiveservices\\speech\\speech_py_impl.py\u001b[0m in \u001b[0;36mrecognize_once\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m  12129\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrecognize_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  12130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 12131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_speech_py_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTranslationRecognizer_recognize_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m  12132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  12133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSystemError\u001b[0m: <built-in function TranslationRecognizer_recognize_once> returned a result with an error set"
     ]
    }
   ],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "speech_key, service_region = \"7d1206ee3a30479db6fa9fdf0154a209\", \"southcentralus\"\n",
    "\n",
    "def translate_speech_to_text():\n",
    "\n",
    "    # Creates an instance of a speech translation config with specified subscription key and service region.\n",
    "    # Replace with your own subscription key and region identifier from here: https://aka.ms/speech/sdkregion\n",
    "    translation_config = speechsdk.translation.SpeechTranslationConfig(subscription=speech_key, region=service_region)\n",
    "\n",
    "    # Sets source and target languages.\n",
    "    # Replace with the languages of your choice, from list found here: https://aka.ms/speech/sttt-languages\n",
    "    fromLanguage = 'ja-JP'\n",
    "    translation_config.speech_recognition_language = fromLanguage\n",
    "    translation_config.add_target_language('de')\n",
    "    translation_config.add_target_language('fr')\n",
    "#     translation_config.add_target_language('ja-JP')\n",
    "\n",
    "    # Creates a translation recognizer using and audio file as input.\n",
    "    recognizer = speechsdk.translation.TranslationRecognizer(translation_config=translation_config)\n",
    "\n",
    "    # Starts translation, and returns after a single utterance is recognized. The end of a\n",
    "    # single utterance is determined by listening for silence at the end or until a maximum of 15\n",
    "    # seconds of audio is processed. It returns the recognized text as well as the translation.\n",
    "    # Note: Since recognize_once() returns only a single utterance, it is suitable only for single\n",
    "    # shot recognition like command or query.\n",
    "    # For long-running multi-utterance recognition, use start_continuous_recognition() instead.\n",
    "    print(\"Say something...\")\n",
    "    result = recognizer.recognize_once()\n",
    "\n",
    "    # Check the result\n",
    "    if result.reason == speechsdk.ResultReason.TranslatedSpeech:\n",
    "        print(\"RECOGNIZED '{}': {}\".format(fromLanguage, result.text))\n",
    "        print(\"TRANSLATED into {}: {}\".format('de', result.translations['de']))\n",
    "        print(\"TRANSLATED into {}: {}\".format('fr', result.translations['fr']))\n",
    "#         print(\"TRANSLATED into {}: {}\".format('fr', result.translations['ja']))\n",
    "    elif result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "        print(\"RECOGNIZED: {} (text could not be translated)\".format(result.text))\n",
    "    elif result.reason == speechsdk.ResultReason.NoMatch:\n",
    "        print(\"NOMATCH: Speech could not be recognized: {}\".format(result.no_match_details))\n",
    "    elif result.reason == speechsdk.ResultReason.Canceled:\n",
    "        print(\"CANCELED: Reason={}\".format(result.cancellation_details.reason))\n",
    "        if result.cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "            print(\"CANCELED: ErrorDetails={}\".format(result.cancellation_details.error_details))\n",
    "\n",
    "translate_speech_to_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 語音轉成多國語音服務操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Say something...\n",
      "AUDIO SYNTHESIZED: 50444 byte(s) \n",
      "AUDIO SYNTHESIZED: 0 byte(s) (COMPLETED)\n",
      "RECOGNIZED 'en-US': Hello.\n",
      "TRANSLATED into ja-JP: こんにちは。\n"
     ]
    }
   ],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "speech_key, service_region = \"7d1206ee3a30479db6fa9fdf0154a209\", \"southcentralus\"\n",
    "\n",
    "def translate_speech_to_speech():\n",
    "\n",
    "    # Creates an instance of a speech translation config with specified subscription key and service region.\n",
    "    # Replace with your own subscription key and region identifier from here: https://aka.ms/speech/sdkregion\n",
    "    translation_config = speechsdk.translation.SpeechTranslationConfig(subscription=speech_key, region=service_region)\n",
    "\n",
    "    # Sets source and target languages.\n",
    "    # Replace with the languages of your choice, from list found here: https://aka.ms/speech/sttt-languages\n",
    "    fromLanguage = 'en-US'\n",
    "    toLanguage = 'ja-JP'\n",
    "    translation_config.speech_recognition_language = fromLanguage\n",
    "    translation_config.add_target_language(toLanguage)\n",
    "\n",
    "    # Sets the synthesis output voice name.\n",
    "    # Replace with the languages of your choice, from list found here: https://aka.ms/speech/tts-languages\n",
    "    translation_config.voice_name = \"ja-JP-NanamiNeural\"\n",
    "\n",
    "    # Creates a translation recognizer using and audio file as input.\n",
    "    recognizer = speechsdk.translation.TranslationRecognizer(translation_config=translation_config)\n",
    "\n",
    "    # Prepare to handle the synthesized audio data.\n",
    "    def synthesis_callback(evt):\n",
    "        size = len(evt.result.audio)\n",
    "        print('AUDIO SYNTHESIZED: {} byte(s) {}'.format(size, '(COMPLETED)' if size == 0 else ''))\n",
    "        if size > 0:\n",
    "            file = open('translation.wav', 'wb+')\n",
    "            file.write(evt.result.audio)\n",
    "            file.close()\n",
    "\n",
    "\n",
    "    recognizer.synthesizing.connect(synthesis_callback)\n",
    "\n",
    "    # Starts translation, and returns after a single utterance is recognized. The end of a\n",
    "    # single utterance is determined by listening for silence at the end or until a maximum of 15\n",
    "    # seconds of audio is processed. It returns the recognized text as well as the translation.\n",
    "    # Note: Since recognize_once() returns only a single utterance, it is suitable only for single\n",
    "    # shot recognition like command or query.\n",
    "    # For long-running multi-utterance recognition, use start_continuous_recognition() instead.\n",
    "    print(\"Say something...\")\n",
    "    result = recognizer.recognize_once()\n",
    "\n",
    "    # Check the result\n",
    "    if result.reason == speechsdk.ResultReason.TranslatedSpeech:\n",
    "        print(\"RECOGNIZED '{}': {}\".format(fromLanguage, result.text))\n",
    "        print(\"TRANSLATED into {}: {}\".format(toLanguage, result.translations['ja']))\n",
    "    elif result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "        print(\"RECOGNIZED: {} (text could not be translated)\".format(result.text))\n",
    "    elif result.reason == speechsdk.ResultReason.NoMatch:\n",
    "        print(\"NOMATCH: Speech could not be recognized: {}\".format(result.no_match_details))\n",
    "    elif result.reason == speechsdk.ResultReason.Canceled:\n",
    "        print(\"CANCELED: Reason={}\".format(result.cancellation_details.reason))\n",
    "        if result.cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "            print(\"CANCELED: ErrorDetails={}\".format(result.cancellation_details.error_details))\n",
    "\n",
    "translate_speech_to_speech()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

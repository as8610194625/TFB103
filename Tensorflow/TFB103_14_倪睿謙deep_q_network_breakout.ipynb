{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jWRbKVLZoMRq"
   },
   "source": [
    "# Deep Q-Learning for Atari Breakout\n",
    "\n",
    "**Author:** [Jacob Chapman](https://twitter.com/jacoblchapman) and [Mathias Lechner](https://twitter.com/MLech20)<br>\n",
    "**Date created:** 2020/05/23<br>\n",
    "**Last modified:** 2020/06/17<br>\n",
    "**Description:** Play Atari Breakout with a Deep Q-Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RgvxYlqkoMR7"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This script shows an implementation of Deep Q-Learning on the\n",
    "`BreakoutNoFrameskip-v4` environment.\n",
    "\n",
    "### Deep Q-Learning\n",
    "\n",
    "As an agent takes actions and moves through an environment, it learns to map\n",
    "the observed state of the environment to an action. An agent will choose an action\n",
    "in a given state based on a \"Q-value\", which is a weighted reward based on the\n",
    "expected highest long-term reward. A Q-Learning Agent learns to perform its\n",
    "task such that the recommended action maximizes the potential future rewards.\n",
    "This method is considered an \"Off-Policy\" method,\n",
    "meaning its Q values are updated assuming that the best action was chosen, even\n",
    "if the best action was not chosen.\n",
    "\n",
    "### Atari Breakout\n",
    "\n",
    "In this environment, a board moves along the bottom of the screen returning a ball that\n",
    "will destroy blocks at the top of the screen.\n",
    "The aim of the game is to remove all blocks and breakout of the\n",
    "level. The agent must learn to control the board by moving left and right, returning the\n",
    "ball and removing all the blocks without the ball passing the board.\n",
    "\n",
    "### Note\n",
    "\n",
    "The Deepmind paper trained for \"a total of 50 million frames (that is, around 38 days of\n",
    "game experience in total)\". However this script will give good results at around 10\n",
    "million frames which are processed in less than 24 hours on a modern machine.\n",
    "\n",
    "### References\n",
    "\n",
    "- [Q-Learning](https://link.springer.com/content/pdf/10.1007/BF00992698.pdf)\n",
    "- [Deep Q-Learning](https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7OxxSARoMSA"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sAu2h1DSpAPW",
    "outputId": "0c4bc62a-fb01-486c-cb37-7405562fd35a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting baselines\n",
      "  Cloning git://github.com/openai/baselines.git (to revision 8e56dd) to /tmp/pip-install-fguii877/baselines_68e8a250f68041858774c9fc82da8cb7\n",
      "  Running command git clone --filter=blob:none -q git://github.com/openai/baselines.git /tmp/pip-install-fguii877/baselines_68e8a250f68041858774c9fc82da8cb7\n",
      "\u001b[33m  WARNING: Did not find branch or tag '8e56dd', assuming revision or ref.\u001b[0m\n",
      "  Running command git checkout -q 8e56dd\n",
      "  Resolved git://github.com/openai/baselines.git to commit 8e56dd\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting gym\n",
      "  Downloading gym-0.21.0.tar.gz (1.5 MB)\n",
      "     |████████████████████████████████| 1.5 MB 1.3 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (from baselines) (1.7.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from baselines) (4.62.3)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from baselines) (1.1.0)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.9/site-packages (from baselines) (0.3.4)\n",
      "Collecting progressbar2\n",
      "  Downloading progressbar2-3.55.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting mpi4py\n",
      "  Downloading mpi4py-3.1.1.tar.gz (2.4 MB)\n",
      "     |████████████████████████████████| 2.4 MB 4.5 MB/s            \n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing wheel metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: cloudpickle in /opt/conda/lib/python3.9/site-packages (from baselines) (2.0.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from baselines) (8.0.3)\n",
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.5.4.58-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.3 MB)\n",
      "     |████████████████████████████████| 60.3 MB 14.9 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.9/site-packages (from gym->baselines) (1.19.5)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from progressbar2->baselines) (1.15.0)\n",
      "Collecting python-utils>=2.3.0\n",
      "  Downloading python_utils-2.5.6-py2.py3-none-any.whl (12 kB)\n",
      "Building wheels for collected packages: baselines, gym, mpi4py\n",
      "  Building wheel for baselines (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for baselines: filename=baselines-0.1.5-py3-none-any.whl size=199009 sha256=317957eb279406caff7003b0b76e98bc52c9fb07c3c823826e196206d9388e38\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-lfkoku0z/wheels/f3/d9/14/3985c2a5b06f7291835b14a818a9604d66e794d77ddfa07eea\n",
      "  Building wheel for gym (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.21.0-py3-none-any.whl size=1616821 sha256=af66d21f999d019841a0b7879e0d40ea45ec313bb7046df50c20f9b638646974\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/b3/50/6c/0a82c1358b4da2dbd9c1bb17e0f89467db32812ab236dbf6d5\n",
      "  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
      "   command: /opt/conda/bin/python3.9 /opt/conda/lib/python3.9/site-packages/pip/_vendor/pep517/in_process/_in_process.py build_wheel /tmp/tmpld1ocein\n",
      "       cwd: /tmp/pip-install-fguii877/mpi4py_20c1ef50bfb449b99e6e3e4247ce8446\n",
      "  Complete output (135 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_src\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build/lib.linux-x86_64-3.9\n",
      "  creating build/lib.linux-x86_64-3.9/mpi4py\n",
      "  copying src/mpi4py/__main__.py -> build/lib.linux-x86_64-3.9/mpi4py\n",
      "  copying src/mpi4py/run.py -> build/lib.linux-x86_64-3.9/mpi4py\n",
      "  copying src/mpi4py/bench.py -> build/lib.linux-x86_64-3.9/mpi4py\n",
      "  copying src/mpi4py/__init__.py -> build/lib.linux-x86_64-3.9/mpi4py\n",
      "  creating build/lib.linux-x86_64-3.9/mpi4py/futures\n",
      "  copying src/mpi4py/futures/__main__.py -> build/lib.linux-x86_64-3.9/mpi4py/futures\n",
      "  copying src/mpi4py/futures/_base.py -> build/lib.linux-x86_64-3.9/mpi4py/futures\n",
      "  copying src/mpi4py/futures/aplus.py -> build/lib.linux-x86_64-3.9/mpi4py/futures\n",
      "  copying src/mpi4py/futures/pool.py -> build/lib.linux-x86_64-3.9/mpi4py/futures\n",
      "  copying src/mpi4py/futures/_core.py -> build/lib.linux-x86_64-3.9/mpi4py/futures\n",
      "  copying src/mpi4py/futures/_lib.py -> build/lib.linux-x86_64-3.9/mpi4py/futures\n",
      "  copying src/mpi4py/futures/__init__.py -> build/lib.linux-x86_64-3.9/mpi4py/futures\n",
      "  copying src/mpi4py/futures/server.py -> build/lib.linux-x86_64-3.9/mpi4py/futures\n",
      "  creating build/lib.linux-x86_64-3.9/mpi4py/util\n",
      "  copying src/mpi4py/util/pkl5.py -> build/lib.linux-x86_64-3.9/mpi4py/util\n",
      "  copying src/mpi4py/util/dtlib.py -> build/lib.linux-x86_64-3.9/mpi4py/util\n",
      "  copying src/mpi4py/util/__init__.py -> build/lib.linux-x86_64-3.9/mpi4py/util\n",
      "  copying src/mpi4py/py.typed -> build/lib.linux-x86_64-3.9/mpi4py\n",
      "  copying src/mpi4py/run.pyi -> build/lib.linux-x86_64-3.9/mpi4py\n",
      "  copying src/mpi4py/__main__.pyi -> build/lib.linux-x86_64-3.9/mpi4py\n",
      "  copying src/mpi4py/dl.pyi -> build/lib.linux-x86_64-3.9/mpi4py\n",
      "  copying src/mpi4py/__init__.pyi -> build/lib.linux-x86_64-3.9/mpi4py\n",
      "  copying src/mpi4py/MPI.pyi -> build/lib.linux-x86_64-3.9/mpi4py\n",
      "  copying src/mpi4py/bench.pyi -> build/lib.linux-x86_64-3.9/mpi4py\n",
      "  copying src/mpi4py/__init__.pxd -> build/lib.linux-x86_64-3.9/mpi4py\n",
      "  copying src/mpi4py/libmpi.pxd -> build/lib.linux-x86_64-3.9/mpi4py\n",
      "  copying src/mpi4py/MPI.pxd -> build/lib.linux-x86_64-3.9/mpi4py\n",
      "  creating build/lib.linux-x86_64-3.9/mpi4py/include\n",
      "  creating build/lib.linux-x86_64-3.9/mpi4py/include/mpi4py\n",
      "  copying src/mpi4py/include/mpi4py/mpi4py.MPI_api.h -> build/lib.linux-x86_64-3.9/mpi4py/include/mpi4py\n",
      "  copying src/mpi4py/include/mpi4py/mpi4py.MPI.h -> build/lib.linux-x86_64-3.9/mpi4py/include/mpi4py\n",
      "  copying src/mpi4py/include/mpi4py/mpi4py.h -> build/lib.linux-x86_64-3.9/mpi4py/include/mpi4py\n",
      "  copying src/mpi4py/include/mpi4py/mpi4py.i -> build/lib.linux-x86_64-3.9/mpi4py/include/mpi4py\n",
      "  copying src/mpi4py/include/mpi4py/mpi.pxi -> build/lib.linux-x86_64-3.9/mpi4py/include/mpi4py\n",
      "  copying src/mpi4py/futures/aplus.pyi -> build/lib.linux-x86_64-3.9/mpi4py/futures\n",
      "  copying src/mpi4py/futures/__main__.pyi -> build/lib.linux-x86_64-3.9/mpi4py/futures\n",
      "  copying src/mpi4py/futures/__init__.pyi -> build/lib.linux-x86_64-3.9/mpi4py/futures\n",
      "  copying src/mpi4py/futures/pool.pyi -> build/lib.linux-x86_64-3.9/mpi4py/futures\n",
      "  copying src/mpi4py/futures/_core.pyi -> build/lib.linux-x86_64-3.9/mpi4py/futures\n",
      "  copying src/mpi4py/futures/_lib.pyi -> build/lib.linux-x86_64-3.9/mpi4py/futures\n",
      "  copying src/mpi4py/futures/server.pyi -> build/lib.linux-x86_64-3.9/mpi4py/futures\n",
      "  copying src/mpi4py/util/dtlib.pyi -> build/lib.linux-x86_64-3.9/mpi4py/util\n",
      "  copying src/mpi4py/util/pkl5.pyi -> build/lib.linux-x86_64-3.9/mpi4py/util\n",
      "  copying src/mpi4py/util/__init__.pyi -> build/lib.linux-x86_64-3.9/mpi4py/util\n",
      "  running build_clib\n",
      "  MPI configuration: [mpi] from 'mpi.cfg'\n",
      "  checking for library 'lmpe' ...\n",
      "  gcc -pthread -B /opt/conda/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/include -fPIC -O2 -isystem /opt/conda/include -fPIC -c _configtest.c -o _configtest.o\n",
      "  failure.\n",
      "  removing: _configtest.c _configtest.o\n",
      "  building 'mpe' dylib library\n",
      "  creating build/temp.linux-x86_64-3.9\n",
      "  creating build/temp.linux-x86_64-3.9/src\n",
      "  creating build/temp.linux-x86_64-3.9/src/lib-pmpi\n",
      "  gcc -pthread -B /opt/conda/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/include -fPIC -O2 -isystem /opt/conda/include -fPIC -c src/lib-pmpi/mpe.c -o build/temp.linux-x86_64-3.9/src/lib-pmpi/mpe.o\n",
      "  warning: build_clib: command 'gcc' failed: No such file or directory\n",
      "  \n",
      "  warning: build_clib: building optional library \"mpe\" failed\n",
      "  \n",
      "  checking for library 'vt-mpi' ...\n",
      "  gcc -pthread -B /opt/conda/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/include -fPIC -O2 -isystem /opt/conda/include -fPIC -c _configtest.c -o _configtest.o\n",
      "  failure.\n",
      "  removing: _configtest.c _configtest.o\n",
      "  checking for library 'vt.mpi' ...\n",
      "  gcc -pthread -B /opt/conda/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/include -fPIC -O2 -isystem /opt/conda/include -fPIC -c _configtest.c -o _configtest.o\n",
      "  failure.\n",
      "  removing: _configtest.c _configtest.o\n",
      "  building 'vt' dylib library\n",
      "  gcc -pthread -B /opt/conda/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/include -fPIC -O2 -isystem /opt/conda/include -fPIC -c src/lib-pmpi/vt.c -o build/temp.linux-x86_64-3.9/src/lib-pmpi/vt.o\n",
      "  warning: build_clib: command 'gcc' failed: No such file or directory\n",
      "  \n",
      "  warning: build_clib: building optional library \"vt\" failed\n",
      "  \n",
      "  checking for library 'vt-mpi' ...\n",
      "  gcc -pthread -B /opt/conda/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/include -fPIC -O2 -isystem /opt/conda/include -fPIC -c _configtest.c -o _configtest.o\n",
      "  failure.\n",
      "  removing: _configtest.c _configtest.o\n",
      "  checking for library 'vt.mpi' ...\n",
      "  gcc -pthread -B /opt/conda/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/include -fPIC -O2 -isystem /opt/conda/include -fPIC -c _configtest.c -o _configtest.o\n",
      "  failure.\n",
      "  removing: _configtest.c _configtest.o\n",
      "  building 'vt-mpi' dylib library\n",
      "  gcc -pthread -B /opt/conda/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/include -fPIC -O2 -isystem /opt/conda/include -fPIC -c src/lib-pmpi/vt-mpi.c -o build/temp.linux-x86_64-3.9/src/lib-pmpi/vt-mpi.o\n",
      "  warning: build_clib: command 'gcc' failed: No such file or directory\n",
      "  \n",
      "  warning: build_clib: building optional library \"vt-mpi\" failed\n",
      "  \n",
      "  checking for library 'vt-hyb' ...\n",
      "  gcc -pthread -B /opt/conda/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/include -fPIC -O2 -isystem /opt/conda/include -fPIC -c _configtest.c -o _configtest.o\n",
      "  failure.\n",
      "  removing: _configtest.c _configtest.o\n",
      "  checking for library 'vt.ompi' ...\n",
      "  gcc -pthread -B /opt/conda/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/include -fPIC -O2 -isystem /opt/conda/include -fPIC -c _configtest.c -o _configtest.o\n",
      "  failure.\n",
      "  removing: _configtest.c _configtest.o\n",
      "  building 'vt-hyb' dylib library\n",
      "  gcc -pthread -B /opt/conda/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/include -fPIC -O2 -isystem /opt/conda/include -fPIC -c src/lib-pmpi/vt-hyb.c -o build/temp.linux-x86_64-3.9/src/lib-pmpi/vt-hyb.o\n",
      "  warning: build_clib: command 'gcc' failed: No such file or directory\n",
      "  \n",
      "  warning: build_clib: building optional library \"vt-hyb\" failed\n",
      "  \n",
      "  running build_ext\n",
      "  MPI configuration: [mpi] from 'mpi.cfg'\n",
      "  checking for dlopen() availability ...\n",
      "  checking for header 'dlfcn.h' ...\n",
      "  gcc -pthread -B /opt/conda/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/include -fPIC -O2 -isystem /opt/conda/include -fPIC -I/opt/conda/include/python3.9 -c _configtest.c -o _configtest.o\n",
      "  failure.\n",
      "  removing: _configtest.c _configtest.o\n",
      "  failure.\n",
      "  checking for library 'dl' ...\n",
      "  gcc -pthread -B /opt/conda/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/include -fPIC -O2 -isystem /opt/conda/include -fPIC -I/opt/conda/include/python3.9 -c _configtest.c -o _configtest.o\n",
      "  failure.\n",
      "  removing: _configtest.c _configtest.o\n",
      "  checking for function 'dlopen' ...\n",
      "  gcc -pthread -B /opt/conda/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/include -fPIC -O2 -isystem /opt/conda/include -fPIC -I/opt/conda/include/python3.9 -c _configtest.c -o _configtest.o\n",
      "  failure.\n",
      "  removing: _configtest.c _configtest.o\n",
      "  building 'mpi4py.dl' extension\n",
      "  gcc -pthread -B /opt/conda/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/include -fPIC -O2 -isystem /opt/conda/include -fPIC -I/opt/conda/include/python3.9 -c src/dynload.c -o build/temp.linux-x86_64-3.9/src/dynload.o\n",
      "  warning: build_ext: command 'gcc' failed: No such file or directory\n",
      "  \n",
      "  warning: build_ext: building optional extension \"mpi4py.dl\" failed\n",
      "  \n",
      "  checking for MPI compile and link ...\n",
      "  gcc -pthread -B /opt/conda/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/include -fPIC -O2 -isystem /opt/conda/include -fPIC -I/opt/conda/include/python3.9 -c _configtest.c -o _configtest.o\n",
      "  failure.\n",
      "  removing: _configtest.c _configtest.o\n",
      "  error: Cannot compile MPI programs. Check your configuration!!!\n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[31m  ERROR: Failed building wheel for mpi4py\u001b[0m\n",
      "\u001b[?25hSuccessfully built baselines gym\n",
      "Failed to build mpi4py\n",
      "\u001b[31mERROR: Could not build wheels for mpi4py, which is required to install pyproject.toml-based projects\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#install baselines\n",
    "!pip install git+git://github.com/openai/baselines.git@8e56dd#egg=baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LhnA68tDpAlg",
    "outputId": "103d9cdb-b9d9-4586-b7b4-1efb8399fc20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: gym[accept-rom-license,atari] in /opt/conda/lib/python3.9/site-packages (0.21.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.9/site-packages (from gym[accept-rom-license,atari]) (1.19.5)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.9/site-packages (from gym[accept-rom-license,atari]) (2.0.0)\n",
      "Collecting ale-py~=0.7.1\n",
      "  Downloading ale_py-0.7.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "     |████████████████████████████████| 1.6 MB 1.1 MB/s            \n",
      "\u001b[?25hCollecting autorom[accept-rom-license]~=0.4.2\n",
      "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: importlib-resources in /opt/conda/lib/python3.9/site-packages (from ale-py~=0.7.1->gym[accept-rom-license,atari]) (5.2.3)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (8.0.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.62.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.26.0)\n",
      "Collecting AutoROM.accept-rom-license\n",
      "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing wheel metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.9/site-packages (from importlib-resources->ale-py~=0.7.1->gym[accept-rom-license,atari]) (3.6.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.26.7)\n",
      "Building wheels for collected packages: AutoROM.accept-rom-license\n",
      "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441026 sha256=ecc7a8869a489cf67794414bdf66a96d617a520b90caf59bc0dc811ecb15f876\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/2b/03/e4/8b662e95b85786a03898fca125d5a9e3fe49337b1eba8fddd2\n",
      "Successfully built AutoROM.accept-rom-license\n",
      "Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n",
      "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.3 autorom-0.4.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#install roms\n",
    "%pip install -U gym>=0.21.0\n",
    "%pip install -U gym[atari,accept-rom-license]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NFP9VtYboMSD",
    "outputId": "fc3d5e37-772c-4991-e33d-442d4f7bb1d7"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'baselines'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_54/633393273.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbaselines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matari_wrappers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_atari\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrap_deepmind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'baselines'"
     ]
    }
   ],
   "source": [
    "from baselines.common.atari_wrappers import make_atari, wrap_deepmind\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "#開始準備\n",
    "# Configuration paramaters for the whole setup\n",
    "seed = 42\n",
    "gamma = 0.99  # Discount factor for past rewards\n",
    "epsilon = 1.0  # Epsilon greedy parameter\n",
    "epsilon_min = 0.1  # Minimum epsilon greedy parameter\n",
    "epsilon_max = 1.0  # Maximum epsilon greedy parameter\n",
    "epsilon_interval = (\n",
    "    epsilon_max - epsilon_min\n",
    ")  # Rate at which to reduce chance of random action being taken\n",
    "batch_size = 32  # Size of batch taken from replay buffer\n",
    "max_steps_per_episode = 10000\n",
    "\n",
    "# Use the Baseline Atari environment because of Deepmind helper functions(官方自己做的ENV)\n",
    "env = make_atari(\"BreakoutNoFrameskip-v4\")\n",
    "# Warp the frames, grey scale, stake four frame and scale to smaller ratio\n",
    "env = wrap_deepmind(env, frame_stack=True, scale=True)\n",
    "env.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gL3TFZoqoMSK"
   },
   "source": [
    "## Implement the Deep Q-Network\n",
    "\n",
    "This network learns an approximation of the Q-table, which is a mapping between\n",
    "the states and actions that an agent will take. For every state we'll have four\n",
    "actions, that can be taken. The environment provides the state, and the action\n",
    "is chosen by selecting the larger of the four Q-values predicted in the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmtNhJd5oMSO"
   },
   "outputs": [],
   "source": [
    "num_actions = 4\n",
    "\n",
    "#Q model製作\n",
    "def create_q_model():\n",
    "    # Network defined by the Deepmind paper\n",
    "    inputs = layers.Input(shape=(84, 84, 4,))\n",
    "\n",
    "    # Convolutions on the frames on the screen\n",
    "    layer1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\")(inputs)\n",
    "    layer2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\")(layer1)\n",
    "    layer3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\")(layer2)\n",
    "\n",
    "    layer4 = layers.Flatten()(layer3)\n",
    "\n",
    "    layer5 = layers.Dense(512, activation=\"relu\")(layer4)\n",
    "    action = layers.Dense(num_actions, activation=\"linear\")(layer5)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=action)\n",
    "\n",
    "\n",
    "# The first model makes the predictions for Q-values which are used to\n",
    "# make a action.\n",
    "model = create_q_model()\n",
    "# Build a target model for the prediction of future rewards.\n",
    "# The weights of a target model get updated every 10000 steps thus when the\n",
    "# loss between the Q-values is calculated the target Q-value is stable.\n",
    "model_target = create_q_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6KgkgseoMSR"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "SQr8Y_bxoMSU",
    "outputId": "06565956-d33c-4455-815e-b8de2a8f7d19"
   },
   "outputs": [],
   "source": [
    "# In the Deepmind paper they use RMSProp however then Adam optimizer\n",
    "# improves training time\n",
    "#開始訓練\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "\n",
    "# Experience replay buffers\n",
    "action_history = []\n",
    "state_history = []\n",
    "state_next_history = []\n",
    "rewards_history = []\n",
    "done_history = []\n",
    "episode_reward_history = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "frame_count = 0\n",
    "# Number of frames to take random action and observe output\n",
    "epsilon_random_frames = 50000\n",
    "# Number of frames for exploration\n",
    "epsilon_greedy_frames = 1000000.0\n",
    "# Maximum replay length\n",
    "# Note: The Deepmind paper suggests 1000000 however this causes memory issues\n",
    "max_memory_length = 100000\n",
    "# Train the model after 4 actions\n",
    "update_after_actions = 4\n",
    "# How often to update the target network\n",
    "update_target_network = 10000\n",
    "# Using huber loss for stability\n",
    "loss_function = keras.losses.Huber()\n",
    "\n",
    "while True:  # Run until solved\n",
    "    state = np.array(env.reset())\n",
    "    episode_reward = 0\n",
    "\n",
    "    for timestep in range(1, max_steps_per_episode):\n",
    "        # env.render(); Adding this line would show the attempts\n",
    "        # of the agent in a pop up window.\n",
    "        frame_count += 1\n",
    "\n",
    "        # Use epsilon-greedy for exploration (有X%的機率會亂做，X%機率正常執行，經常在增強式學習使用)\n",
    "        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n",
    "            # Take random action\n",
    "            action = np.random.choice(num_actions)\n",
    "        else:\n",
    "            # Predict action Q-values\n",
    "            # From environment state\n",
    "            state_tensor = tf.convert_to_tensor(state)\n",
    "            state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "            action_probs = model(state_tensor, training=False)\n",
    "            # Take best action\n",
    "            action = tf.argmax(action_probs[0]).numpy()\n",
    "\n",
    "        # Decay probability of taking random action\n",
    "        epsilon -= epsilon_interval / epsilon_greedy_frames\n",
    "        epsilon = max(epsilon, epsilon_min)\n",
    "\n",
    "        # Apply the sampled action in our environment\n",
    "        state_next, reward, done, _ = env.step(action)\n",
    "        state_next = np.array(state_next)\n",
    "\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Save actions and states in replay buffer\n",
    "        action_history.append(action)\n",
    "        state_history.append(state)\n",
    "        state_next_history.append(state_next)\n",
    "        done_history.append(done)\n",
    "        rewards_history.append(reward)\n",
    "        state = state_next\n",
    "\n",
    "        # Update every fourth frame and once batch size is over 32 (每四個FRAME更新一次)\n",
    "        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
    "\n",
    "            # Get indices of samples for replay buffers\n",
    "            indices = np.random.choice(range(len(done_history)), size=batch_size)\n",
    "\n",
    "            # Using list comprehension to sample from replay buffer\n",
    "            state_sample = np.array([state_history[i] for i in indices])\n",
    "            state_next_sample = np.array([state_next_history[i] for i in indices])\n",
    "            rewards_sample = [rewards_history[i] for i in indices]\n",
    "            action_sample = [action_history[i] for i in indices]\n",
    "            done_sample = tf.convert_to_tensor(\n",
    "                [float(done_history[i]) for i in indices]\n",
    "            )\n",
    "\n",
    "            # Build the updated Q-values for the sampled future states\n",
    "            # Use the target model for stability\n",
    "            future_rewards = model_target.predict(state_next_sample)\n",
    "            # Q value = reward + discount factor * expected future reward\n",
    "            updated_q_values = rewards_sample + gamma * tf.reduce_max(\n",
    "                future_rewards, axis=1\n",
    "            )\n",
    "\n",
    "            # If final frame set the last value to -1\n",
    "            updated_q_values = updated_q_values * (1 - done_sample) - done_sample\n",
    "\n",
    "            # Create a mask so we only calculate loss on the updated Q-values (只計算loss並改動Q-values)\n",
    "            masks = tf.one_hot(action_sample, num_actions)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Train the model on the states and updated Q-values\n",
    "                q_values = model(state_sample)\n",
    "\n",
    "                # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "                # Calculate loss between new Q-value and old Q-value\n",
    "                loss = loss_function(updated_q_values, q_action)\n",
    "\n",
    "            # Backpropagation\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        if frame_count % update_target_network == 0:\n",
    "            # update the the target network with new weights\n",
    "            model_target.set_weights(model.get_weights())\n",
    "            # Log details\n",
    "            template = \"running reward: {:.2f} at episode {}, frame count {}\"\n",
    "            print(template.format(running_reward, episode_count, frame_count))\n",
    "\n",
    "        # Limit the state and reward history\n",
    "        if len(rewards_history) > max_memory_length:\n",
    "            del rewards_history[:1]\n",
    "            del state_history[:1]\n",
    "            del state_next_history[:1]\n",
    "            del action_history[:1]\n",
    "            del done_history[:1]\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Update running reward to check condition for solving\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    if len(episode_reward_history) > 100:\n",
    "        del episode_reward_history[:1]\n",
    "    running_reward = np.mean(episode_reward_history)\n",
    "\n",
    "    episode_count += 1\n",
    "\n",
    "    if running_reward > 40:  # Condition to consider the task solved\n",
    "        print(\"Solved at episode {}!\".format(episode_count))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20OwbMfZoMSo"
   },
   "source": [
    "## Visualizations\n",
    "Before any training:\n",
    "![Imgur](https://i.imgur.com/rRxXF4H.gif)\n",
    "\n",
    "In early stages of training:\n",
    "![Imgur](https://i.imgur.com/X8ghdpL.gif)\n",
    "\n",
    "In later stages of training:\n",
    "![Imgur](https://i.imgur.com/Z1K6qBQ.gif)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TFB103-14-倪睿謙deep_q_network_breakout",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
